{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Module 2.3] 세이지 메이커 사용자 도커 이미지로 인퍼런스\n",
    "\n",
    "## 필수 사항\n",
    "- 이 노트북은 sagemaker_inference_container/container-inference/ 의 아래 두개의 노트북을 먼저 실행해야 합니다.\n",
    "    - 1.1.Build_Docker.ipynb\n",
    "    - 2.1.Package_Model_Artifact.ipynb\n",
    "본 워크샵의 모든 노트북은 `conda_python3` 추가 패키지를 설치하고 모두 이 커널 에서 작업 합니다.\n",
    "\n",
    "\n",
    "## 이 노트북은 아래와 같은 작업을 합니다.\n",
    "\n",
    "- 1. 환경 셋업\n",
    "- 2. 로컬 모드에서 배포 및 추론 테스트\n",
    "- 3. 세이지 메이커 (호스트 모드)에서 배포 및 추론 테스트\n",
    "- 4. 세이지 메이커 신규 모델로 앤드포인트 업데이트\n",
    "- 5. 신규 엔드포인트 생성\n",
    "- 6. 신규 앤드포인트 생성 후에 Bake Time (예: 5분) 지난 후에 기존 엔드포인트 삭제\n",
    "- 7. 마지막 신규 엔드포인트 리소스 제거\n",
    "\n",
    "\n",
    "---    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 기본 세팅\n",
    "사용하는 패키지는 import 시점에 다시 재로딩 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 노트북에서 인퍼런스 테스트를 완료한 티펙트를 가져옵니다.\n",
    "- inference_docker_image: 사용자 정의 도커 이미지\n",
    "- byom_artifact: 추론 코드를 포함한 모델 아티펙트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r inference_docker_image\n",
    "# %store -r artifact_path\n",
    "%store -r byom_artifact\n",
    "%store -r bucket\n",
    "%store -r prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_docker_image: \n",
      " 057716757052.dkr.ecr.us-east-1.amazonaws.com/ncf-sagemaker-inference\n",
      "byom_artifact: \n",
      " s3://sagemaker-us-east-1-057716757052/sm2fraud/new2train2model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(\"inference_docker_image: \\n\", inference_docker_image)\n",
    "print(\"byom_artifact: \\n\", byom_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 로컬 모드에서 배포 및 추론 테스트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Python SDK를 통한 세이지 메이커 모델 생성\n",
    "- 로컬 엔드포인트에 적재될 세이지 메이커 모델을 생성홥니다.\n",
    "- 세이지 메이커 모델은 크게 아래 두가지 요소가 있습니다.\n",
    "    - 인퍼런스 이미지 컨테이너 위치\n",
    "    - 모델 아티펙트 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_endpoint_name:  local-endpoint-2022-10-16-13-24-46\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "suffix = f\"{datetime.today().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# 로컬 노트북 인스턴스에서 사용할 로컬 세션\n",
    "local_session = sagemaker.local.LocalSession()\n",
    "local_session.config = {'local' : {'local_code':True}}\n",
    "instance_type = 'local_gpu'\n",
    "local_model_name = 'sm-local-model-' + str(suffix)\n",
    "local_endpoint_name = 'local-endpoint-' + str(suffix)\n",
    "print(\"local_endpoint_name: \", local_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 로컬 세이지 메이커 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  <sagemaker.model.Model object at 0x7f1aebc57c10>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "def create_sm_model(ecr_image, model_artifact,model_name,  role, session):\n",
    "    model = Model(\n",
    "                name = model_name,\n",
    "                image_uri = ecr_image,\n",
    "                model_data = model_artifact,\n",
    "                role=role,\n",
    "                sagemaker_session= session,\n",
    "                 )\n",
    "    print(\"model: \", model)\n",
    "    return model\n",
    "\n",
    "local_model = create_sm_model(inference_docker_image, byom_artifact, \n",
    "                              local_model_name, role, \n",
    "                              local_session)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 로컬 엔드포인트 생성\n",
    "\n",
    "- instance_type=='local' 시에는 최초 실행시에 인퍼런스 이미지를 다운로드로 하는 약 3분 걸리고, 이후에는 바로 실행이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 0rpq12cibv-algo-1-4gkb0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Collecting nvidia-ml-py3==7.352\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hCollecting pandas==0.24.2\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading pandas-0.24.2-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "     |████████████████████████████████| 10.1 MB 38.9 MB/s            \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hCollecting numpy==1.16.6\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading numpy-1.16.6-cp36-cp36m-manylinux1_x86_64.whl (17.4 MB)\n",
      "     |████████████████████████████████| 17.4 MB 54.4 MB/s            \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hRequirement already satisfied: torch==1.8.1 in /opt/conda/lib/python3.6/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (1.8.1)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Collecting gensim==3.7.1\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "     |████████████████████████████████| 24.2 MB 63.6 MB/s            \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hCollecting tensorboardX==1.6\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading tensorboardX-1.6-py2.py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 73.3 MB/s            \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hRequirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas==0.24.2->-r /opt/ml/model/code/requirements.txt (line 2)) (2021.3)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas==0.24.2->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8.2)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (4.0.1)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from torch==1.8.1->-r /opt/ml/model/code/requirements.txt (line 4)) (0.8)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Collecting smart-open>=1.7.0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading smart_open-6.2.0-py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 9.1 MB/s             \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim==3.7.1->-r /opt/ml/model/code/requirements.txt (line 5)) (1.3.0)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim==3.7.1->-r /opt/ml/model/code/requirements.txt (line 5)) (1.16.0)\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Collecting protobuf>=3.2.0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Downloading protobuf-3.19.6-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 62.5 MB/s            \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19189 sha256=3c3c8c7fdb06d8ed6d4fa7a48ee40f6a31b9894470c23bdb53555cafb17c4923\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Stored in directory: /root/.cache/pip/wheels/7f/26/a3/33f2079871e2bebb3f53a2b21c3ec64129b8efdd18a6263a52\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Successfully built nvidia-ml-py3\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Installing collected packages: smart-open, protobuf, numpy, tensorboardX, pandas, nvidia-ml-py3, gensim\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Attempting uninstall: numpy\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m     Found existing installation: numpy 1.19.1\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m     Uninstalling numpy-1.19.1:\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m       Successfully uninstalled numpy-1.19.1\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m   Attempting uninstall: pandas\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m     Found existing installation: pandas 0.25.0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m     Uninstalling pandas-0.25.0:\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m       Successfully uninstalled pandas-0.25.0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Successfully installed gensim-3.7.1 numpy-1.16.6 nvidia-ml-py3-7.352.0 pandas-0.24.2 protobuf-3.19.6 smart-open-6.2.0 tensorboardX-1.6\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:01,487 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:01,719 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Torchserve version: 0.4.0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Current directory: /\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Temp directory: /tmp\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Number of GPUs: 4\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Number of CPUs: 32\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Max heap size: 30688 M\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Log dir: /logs\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Netty threads: 0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Default workers per model: 4\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Enable metrics API: true\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Workflow Store: /.sagemaker/ts/models\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:01,727 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:01,759 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,053 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,067 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,159 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,159 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,161 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m Model server started.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,490 [WARN ] pool-2-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,671 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:25.0|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,672 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:428.80698013305664|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,673 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:38.20498275756836|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,673 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:8.2|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,673 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:221085.8203125|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,674 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:22368.56640625|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,674 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:10.0|#Level:Host|#hostname:9636777b54bf,timestamp:1665926702\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,784 [INFO ] W-9001-model_1-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,785 [INFO ] W-9001-model_1-stdout MODEL_LOG - [PID]130\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,785 [INFO ] W-9001-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,785 [INFO ] W-9001-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,792 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,804 [INFO ] W-9001-model_1-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,805 [INFO ] W-9000-model_1-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,806 [INFO ] W-9000-model_1-stdout MODEL_LOG - [PID]129\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,806 [INFO ] W-9000-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,806 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,807 [INFO ] W-9000-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,809 [INFO ] W-9000-model_1-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,838 [INFO ] W-9002-model_1-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,839 [INFO ] W-9002-model_1-stdout MODEL_LOG - [PID]131\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,839 [INFO ] W-9002-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,839 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,839 [INFO ] W-9002-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,852 [INFO ] W-9003-model_1-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,852 [INFO ] W-9003-model_1-stdout MODEL_LOG - [PID]132\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,852 [INFO ] W-9002-model_1-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,852 [INFO ] W-9003-model_1-stdout MODEL_LOG - Torch worker started.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,852 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,853 [INFO ] W-9003-model_1-stdout MODEL_LOG - Python runtime: 3.6.13\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,876 [INFO ] W-9003-model_1-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,879 [INFO ] W-9000-model_1-stdout MODEL_LOG - ######## Staring model_fn() ###############\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,880 [INFO ] W-9000-model_1-stdout MODEL_LOG - --> model_dir : /tmp/models/23cdba9b697e45ae92a6f111ac1a4ec8\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,888 [INFO ] W-9001-model_1-stdout MODEL_LOG - ######## Staring model_fn() ###############\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,888 [INFO ] W-9001-model_1-stdout MODEL_LOG - --> model_dir : /tmp/models/23cdba9b697e45ae92a6f111ac1a4ec8\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,898 [INFO ] W-9002-model_1-stdout MODEL_LOG - ######## Staring model_fn() ###############\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,899 [INFO ] W-9002-model_1-stdout MODEL_LOG - --> model_dir : /tmp/models/23cdba9b697e45ae92a6f111ac1a4ec8\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,920 [INFO ] W-9003-model_1-stdout MODEL_LOG - ######## Staring model_fn() ###############\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,920 [INFO ] W-9003-model_1-stdout MODEL_LOG - --> model_dir : /tmp/models/23cdba9b697e45ae92a6f111ac1a4ec8\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,926 [INFO ] W-9000-model_1-stdout MODEL_LOG - model_config_path: :  /opt/ml/model/code/model_config.json\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,959 [INFO ] W-9001-model_1-stdout MODEL_LOG - model_config_path: :  /opt/ml/model/code/model_config.json\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,968 [INFO ] W-9000-model_1-stdout MODEL_LOG - --> model network is loaded\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,968 [INFO ] W-9002-model_1-stdout MODEL_LOG - model_config_path: :  /opt/ml/model/code/model_config.json\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,968 [INFO ] W-9000-model_1-stdout MODEL_LOG - model_file_path: :  {model_file_path}\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:02,980 [INFO ] W-9003-model_1-stdout MODEL_LOG - model_config_path: :  /opt/ml/model/code/model_config.json\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:03,002 [INFO ] W-9001-model_1-stdout MODEL_LOG - --> model network is loaded\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:03,002 [INFO ] W-9001-model_1-stdout MODEL_LOG - model_file_path: :  {model_file_path}\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:03,012 [INFO ] W-9002-model_1-stdout MODEL_LOG - --> model network is loaded\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:03,012 [INFO ] W-9002-model_1-stdout MODEL_LOG - model_file_path: :  {model_file_path}\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:03,023 [INFO ] W-9003-model_1-stdout MODEL_LOG - --> model network is loaded\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:03,023 [INFO ] W-9003-model_1-stdout MODEL_LOG - model_file_path: :  {model_file_path}\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,736 [INFO ] W-9000-model_1-stdout MODEL_LOG - ####### Model is loaded #########\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,749 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2896\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,749 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:3690|#Level:Host|#hostname:9636777b54bf,timestamp:1665926705\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,750 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:45|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,767 [INFO ] W-9003-model_1-stdout MODEL_LOG - ####### Model is loaded #########\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,771 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2873\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,771 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:3709|#Level:Host|#hostname:9636777b54bf,timestamp:1665926705\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,771 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:22|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,772 [INFO ] W-9002-model_1-stdout MODEL_LOG - ####### Model is loaded #########\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,775 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2900\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,775 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:3714|#Level:Host|#hostname:9636777b54bf,timestamp:1665926705\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,775 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:23|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,788 [INFO ] W-9001-model_1-stdout MODEL_LOG - ####### Model is loaded #########\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,791 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2916\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,791 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:3730|#Level:Host|#hostname:9636777b54bf,timestamp:1665926705\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:05,791 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:66|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:06,998 [INFO ] pool-1-thread-5 ACCESS_LOG - /172.18.0.1:35788 \"GET /ping HTTP/1.1\" 200 10\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:06,999 [INFO ] pool-1-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "!CPU times: user 271 ms, sys: 59.3 ms, total: 330 ms\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "local_predictor = local_model.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                         endpoint_name = local_endpoint_name,                                   \n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 데이터 세트 로딩\n",
    "- 로컬에서 저장된 데이터를 가져와서 데이터를 변환 합니다.\n",
    "- batch_size 만큼 데이터를 로딩하는 데이터 로더를 정의 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils \n",
    "train_data, test_data, user_num ,item_num, train_mat = data_utils.load_all(test_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  추론을 위한  데이터 세트 로딩\n",
    "- 전부 데이터를 로딩할 필요가 없지만, 여기서는 기존에 사용한 함수를 이용하기 위해서 전체 데이터를 로드 합니다. \n",
    "    - 실제 데이터로 구현시에는 따로이 로드 함수를 사용하시기를 권장 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of batch_size:  256\n"
     ]
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        # self.epochs = 1        \n",
    "        self.num_ng = 4\n",
    "        self.batch_size = 256\n",
    "        self.test_num_ng = 99\n",
    "        self.factor_num = 32\n",
    "        self.num_layers = 3\n",
    "        self.dropout = 0.0\n",
    "        # self.lr = 0.001\n",
    "        self.top_k = 10\n",
    "        self.out = True\n",
    "        # self.gpu = \"0\"\n",
    "                        \n",
    "args = Params()\n",
    "print(\"# of batch_size: \", args.batch_size)\n",
    "\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "test_dataset = data_utils.NCFData(\n",
    "\t\ttest_data, item_num, train_mat, 0, False)\n",
    "\n",
    "test_loader = data.DataLoader(test_dataset,\n",
    "\t\tbatch_size=args.test_num_ng+1, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론할 Paylaod 하나를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payload:  {'user': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'item': [25, 1064, 174, 2791, 3373, 269, 2678, 1902, 3641, 1216, 915, 3672, 2803, 2344, 986, 3217, 2824, 2598, 464, 2340, 1952, 1855, 1353, 1547, 3487, 3293, 1541, 2414, 2728, 340, 1421, 1963, 2545, 972, 487, 3463, 2727, 1135, 3135, 128, 175, 2423, 1974, 2515, 3278, 3079, 1527, 2182, 1018, 2800, 1830, 1539, 617, 247, 3448, 1699, 1420, 2487, 198, 811, 1010, 1423, 2840, 1770, 881, 1913, 1803, 1734, 3326, 1617, 224, 3352, 1869, 1182, 1331, 336, 2517, 1721, 3512, 3656, 273, 1026, 1991, 2190, 998, 3386, 3369, 185, 2822, 864, 2854, 3067, 58, 2551, 2333, 2688, 3703, 1300, 1924, 3118]}\n"
     ]
    }
   ],
   "source": [
    "for user, item, label in test_loader:   \n",
    "    user_np = user.detach().cpu().numpy()\n",
    "    item_np = item.detach().cpu().numpy()            \n",
    "    break\n",
    "payload = {'user':user_np.tolist(), 'item':item_np.tolist()}\n",
    "print(\"payload: \", payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엔드 포인트 추론\n",
    "- Boto3 invoke_endpoint() 로 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "if instance_type == 'local_gpu':\n",
    "    runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "    endpoint_name = local_endpoint_name\n",
    "else:\n",
    "    runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [중요] JSON type 의 payload 를 String 으로 직렬화 해서 제공 함.\n",
    "```python\n",
    "payload_dump = json.dumps(payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,406 [INFO ] W-9000-model_1-stdout MODEL_LOG - #### input_fn starting ######\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,406 [INFO ] W-9000-model_1-stdout MODEL_LOG - content_type: application/json\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,406 [INFO ] W-9000-model_1-stdout MODEL_LOG - #### type of input data: <class 'str'>\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,406 [INFO ] W-9000-model_1-stdout MODEL_LOG - #### predict_fn starting ######\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,407 [INFO ] W-9000-model_1-stdout MODEL_LOG - #### type of input data: <class 'list'>\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,823 [INFO ] W-9000-model_1-stdout MODEL_LOG - recommends:  [273, 174, 128, 25, 175, 1331, 1182, 1064, 464, 1539]\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,823 [INFO ] W-9000-model_1-stdout MODEL_METRICS - PredictionTime.Milliseconds:417.54|#ModelName:model,Level:Model|#hostname:9636777b54bf,requestID:c41fb1ec-5cce-4c79-b67a-4fccecda7789,timestamp:1665926720\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,825 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 420\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,825 [INFO ] W-9000-model_1 ACCESS_LOG - /172.18.0.1:47526 \"POST /invocations HTTP/1.1\" 200 428\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,826 [INFO ] W-9000-model_1 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,826 [INFO ] W-9000-model_1 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "\u001b[36m0rpq12cibv-algo-1-4gkb0 |\u001b[0m 2022-10-16 13:25:20,827 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:1|#Level:Host|#hostname:9636777b54bf,timestamp:null\n",
      "--- 0.4929959774017334 seconds ---\n",
      "result:  ['[273, 174, 128, 25, 175, 1331, 1182, 1064, 464, 1539]']\n"
     ]
    }
   ],
   "source": [
    "import json, time\n",
    "from inference_utils import invoke_endpoint\n",
    "payload_dump = json.dumps(payload)\n",
    "\n",
    "start_time = time.time()\n",
    "result = invoke_endpoint(runtime_client, endpoint_name, \n",
    "                         payload_dump,\n",
    "                         content_type='application/json'\n",
    "                        )\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로컬 엔드 포인트 삭제\n",
    "- 기존에 생성한 세이지 메이커 모델, 앤드포인트 컨피그, 앤드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "--- Deleted model: sm-local-model-2022-10-16-13-24-46\n",
      "--- Deleted endpoint: local-endpoint-2022-10-16-13-24-46\n",
      "--- Deleted endpoint_config: local-endpoint-2022-10-16-13-24-46\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import delete_endpoint\n",
    "\n",
    "client = sagemaker.local.LocalSagemakerClient()\n",
    "delete_endpoint(client, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 세이지 메이커 (호스트 모드)에서 배포 및 추론 테스트\n",
    "- 위의 단계에 (로컬 앤드포인트)에서 추론 테스트가 완료 되었기에, 프러덕션을 가정하고 배포 및 추론을 합니다.\n",
    "- [중요] 기존의 SageMaker Python SDK 대신에 Boto3 를 사용합니다.\n",
    "    -  참조: [Boto3 API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.describe_endpoint_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker 호스트 세션 및 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_model_name:  sm-ncf-model-2022-10-16-13-24-46\n",
      "inference_image:  057716757052.dkr.ecr.us-east-1.amazonaws.com/ncf-sagemaker-inference\n",
      "model_artifact:  s3://sagemaker-us-east-1-057716757052/sm2fraud/new2train2model/model.tar.gz\n",
      "endpoint_name:  sm-new-ncf-endpont-2022-10-16-13-24-46\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "# SM 세션에서 사용 리소스 제거에 사용\n",
    "sm_client = boto3.client('sagemaker')\n",
    "sm_runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "sm_model_name = 'sm-ncf-model-' + str(suffix)\n",
    "endpoint_name = 'sm-new-ncf-endpont-' + str(suffix)\n",
    "print(\"sm_model_name: \", sm_model_name)\n",
    "print(\"inference_image: \", inference_docker_image)\n",
    "print(\"model_artifact: \", byom_artifact)\n",
    "print(\"endpoint_name: \", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세이지 메이커 모델 생성\n",
    "- 세이지 메이커에 Model이 존재하지 않으면, 모델을 생성 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_model API response: \n",
      " {'ModelArn': 'arn:aws:sagemaker:us-east-1:057716757052:model/sm-ncf-model-2022-10-16-13-24-46', 'ResponseMetadata': {'RequestId': 'b3e599f4-4622-47a2-b880-a702a541786b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'b3e599f4-4622-47a2-b880-a702a541786b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '94', 'date': 'Sun, 16 Oct 2022 13:25:31 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import create_sm_model                \n",
    "\n",
    "create_model_api_response = create_sm_model(sm_client, sm_model_name, \n",
    "                                            inference_docker_image, \n",
    "                                            byom_artifact, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세이지 메이커 앤드포인트 컨피그 생성\n",
    "- endpoint_config  를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-ncf-model-2022-10-16-13-24-46-config is created\n",
      "sm_endconfig_name:  sm-ncf-model-2022-10-16-13-24-46-config\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import create_endpoint_config\n",
    "\n",
    "instance_type = 'ml.g4dn.xlarge'\n",
    "sm_endconfig_name = create_endpoint_config(sm_client=sm_client, \n",
    "                                           instance_type=instance_type, \n",
    "                                           sm_model_name =sm_model_name, \n",
    "                                           verbose=False )\n",
    "    \n",
    "print(\"sm_endconfig_name: \", sm_endconfig_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 세이지 메이커 앤드포인트 생성\n",
    "- 약 8분 정도 소요 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing_configs: \n",
      " [{'EndpointConfigName': 'sm-ncf-model-2022-10-16-13-24-46-config', 'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/sm-ncf-model-2022-10-16-13-24-46-config', 'CreationTime': datetime.datetime(2022, 10, 16, 13, 25, 32, 180000, tzinfo=tzlocal())}]\n",
      "existing_endpoints: \n",
      " []\n",
      "Creating endpoint\n",
      "Endpoint status is creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: InService\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import create_sm_endpoint\n",
    "\n",
    "create_sm_endpoint(sm_client=sm_client, instance_type= instance_type, \n",
    "                   endpoint_config_name =sm_endconfig_name, \n",
    "                   endpoint_name = endpoint_name, verbose=True )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker 엔드포인트 추론 테스트\n",
    "- 신규로 생성한 앤드포인트가 추론이 달 되는지를 확인하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.4708614349365234 seconds ---\n",
      "result:  ['[273, 174, 128, 25, 175, 1331, 1182, 1064, 464, 1539]']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "if instance_type == 'local_gpu':\n",
    "    runtime_client = sagemaker.local.LocalSagemakerRuntimeClient()    \n",
    "    endpoint_name = local_endpoint_name\n",
    "else:\n",
    "    runtime_client = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "\n",
    "import json, time\n",
    "from inference_utils import invoke_endpoint\n",
    "payload_dump = json.dumps(payload)\n",
    "\n",
    "start_time = time.time()\n",
    "result = invoke_endpoint(runtime_client, endpoint_name, \n",
    "                         payload_dump,\n",
    "                         content_type='application/json'\n",
    "                        )\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print('result: ', result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 세이지 메이커 신규 모델로 앤드포인트 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### current_endpoint_name objects\n",
    "- 기존의 앤드포인트의 오브젝트 정보 확인\n",
    "    - 앤드포인트 이름\n",
    "    - 앤드포인트 컨피그 이름\n",
    "    - 세이지 메이커 모델 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import show_inference_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: \n",
      " sm-new-ncf-endpont-2022-10-16-13-24-46\n",
      "endpoint_config: \n",
      " sm-ncf-model-2022-10-16-13-24-46-config\n",
      "model_name: \n",
      " sm-ncf-model-2022-10-16-13-24-46\n"
     ]
    }
   ],
   "source": [
    "show_inference_objects(sm_client, endpoint_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### new_endpoint_name objects\n",
    "- 위와 동일하게 신규 생성 앤드포인트의 오브젝트 이름을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. 신규 세이지 메이커 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_new_model_name:  sm-ncf-model-2022-10-16-13-30-35\n",
      "byom_new_artifact:  s3://sagemaker-us-east-1-057716757052/sm2fraud/new2train2model/model.tar.gz\n",
      "create_model API response: \n",
      " {'ModelArn': 'arn:aws:sagemaker:us-east-1:057716757052:model/sm-ncf-model-2022-10-16-13-30-35', 'ResponseMetadata': {'RequestId': 'f3f3250b-431f-413f-be85-581b2032b1b2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f3f3250b-431f-413f-be85-581b2032b1b2', 'content-type': 'application/x-amz-json-1.1', 'content-length': '94', 'date': 'Sun, 16 Oct 2022 13:30:36 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "new_suffix = f\"{datetime.today().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "sm_new_model_name = 'sm-ncf-model-' + str(new_suffix)\n",
    "\n",
    "byom_new_artifact = byom_artifact\n",
    "print(\"sm_new_model_name: \", sm_new_model_name)\n",
    "print(\"byom_new_artifact: \", byom_new_artifact)\n",
    "\n",
    "create_model_api_response = create_sm_model(sm_client, sm_new_model_name, \n",
    "                                            inference_docker_image, byom_new_artifact, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. 신규 엔드포인트 컨피그 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-ncf-model-2022-10-16-13-30-35-config is created\n",
      "sm_new_endconfig_name:  sm-ncf-model-2022-10-16-13-30-35-config\n"
     ]
    }
   ],
   "source": [
    "sm_new_endconfig_name = create_endpoint_config(sm_client=sm_client, \n",
    "                                           instance_type=instance_type, \n",
    "                                           sm_model_name =sm_new_model_name, \n",
    "                                           verbose=False )\n",
    "    \n",
    "print(\"sm_new_endconfig_name: \", sm_new_endconfig_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 앤드포인트 업데이트\n",
    "- 아래의 업데이트는 약 8분 정도가 소요 됩니다.\n",
    "- 내부적으로 EC2 인스턴스 생성하고 배포하는 단계를 가집니다.\n",
    "- 8분 동안 업데이트시에도 기존의 앤드포인트로 서비스가 가능합니다.\n",
    "- 업데이트 이후에는 업데이트된 앤드포인트에서 추론이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: \n",
      " sm-new-ncf-endpont-2022-10-16-13-24-46\n",
      "new_endpoint_config_name:  sm-ncf-model-2022-10-16-13-30-35-config\n",
      "current_endpoint_config_name:  sm-ncf-model-2022-10-16-13-24-46-config\n",
      "Swapping is done\n",
      "changed_endpoint_config_name: \n",
      " sm-ncf-model-2022-10-16-13-24-46-config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:057716757052:endpoint/sm-new-ncf-endpont-2022-10-16-13-24-46',\n",
       " 'ResponseMetadata': {'RequestId': '8df019ba-0fc5-4ff7-91ff-b93fd2a8986a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8df019ba-0fc5-4ff7-91ff-b93fd2a8986a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '106',\n",
       "   'date': 'Sun, 16 Oct 2022 13:30:36 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inference_utils import update_sm_endpoint\n",
    "\n",
    "response = update_sm_endpoint(sm_client, endpoint_name, sm_new_endconfig_name)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "\n",
    "while status == 'Updating':\n",
    "    print(\"endpoint status is \", status)    \n",
    "    time.sleep(30)\n",
    "    response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = response['EndpointStatus']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint_name: \n",
      " sm-new-ncf-endpont-2022-10-16-13-24-46\n",
      "endpoint_config: \n",
      " sm-ncf-model-2022-10-16-13-30-35-config\n",
      "model_name: \n",
      " sm-ncf-model-2022-10-16-13-30-35\n"
     ]
    }
   ],
   "source": [
    "show_inference_objects(sm_client, endpoint_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 신규 엔드포인트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_new_model_name:  sm-ncf-model-2022-10-16-13-30-35\n",
      "sm_new_endconfig_name:  sm-ncf-model-2022-10-16-13-30-35-config\n",
      "byom_new_artifact:  s3://sagemaker-us-east-1-057716757052/sm2fraud/new2train2model/model.tar.gz\n",
      "new_endpoint_name:  sm-new-ncf-endpont-2022-10-16-13-30-35\n"
     ]
    }
   ],
   "source": [
    "new_endpoint_name = 'sm-new-ncf-endpont-' + str(new_suffix)\n",
    "print(\"sm_new_model_name: \", sm_new_model_name)\n",
    "print(\"sm_new_endconfig_name: \", sm_new_endconfig_name)\n",
    "print(\"byom_new_artifact: \", byom_new_artifact)\n",
    "print(\"new_endpoint_name: \", new_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing_configs: \n",
      " [{'EndpointConfigName': 'sm-ncf-model-2022-10-16-13-30-35-config', 'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:057716757052:endpoint-config/sm-ncf-model-2022-10-16-13-30-35-config', 'CreationTime': datetime.datetime(2022, 10, 16, 13, 30, 36, 596000, tzinfo=tzlocal())}]\n",
      "existing_endpoints: \n",
      " []\n",
      "Creating endpoint\n",
      "Endpoint status is creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: InService\n"
     ]
    }
   ],
   "source": [
    "create_sm_endpoint(sm_client=sm_client, instance_type= instance_type, \n",
    "                   endpoint_config_name =sm_new_endconfig_name, \n",
    "                   endpoint_name = new_endpoint_name, verbose=True )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 신규 앤드포인트 생성 후에 Bake Time (예: 5분) 지난 후에 기존 엔드포인트 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: \n",
      " sm-ncf-model-2022-10-16-13-30-35\n",
      "EndpointConfigName: \n",
      " sm-ncf-model-2022-10-16-13-30-35-config\n",
      "endpoint_name: \n",
      " sm-new-ncf-endpont-2022-10-16-13-24-46\n",
      "--- Deleted endpoint_config: sm-ncf-model-2022-10-16-13-30-35-config\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import delete_endpoint_detail\n",
    "delete_endpoint_detail(client = sm_client, \n",
    "                endpoint_name = endpoint_name,\n",
    "                is_delete=True, \n",
    "                is_del_model=False, \n",
    "                is_del_endconfig=False,\n",
    "                is_del_endpoint=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 마지막 신규 엔드포인트 리소스 제거\n",
    "- 첫번째는 실제 리소스를 제제거하지 않고, 관련 오브젝트만을 보여 줍니다.\n",
    "- 두번째는 신규 생성한 세이지메이커 모델, 엔드포인트 컨피그는 지우지 않고, 앤프포인트만 삭제 합니다.\n",
    "    - 이유는 업데이트된 앤드포인트에서 사용하기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_utils import delete_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: \n",
      " sm-ncf-model-2022-10-16-13-30-35\n",
      "EndpointConfigName: \n",
      " sm-ncf-model-2022-10-16-13-30-35-config\n",
      "endpoint_name: \n",
      " sm-new-ncf-endpont-2022-10-16-13-30-35\n",
      "--- Deleted endpoint: sm-new-ncf-endpont-2022-10-16-13-30-35\n",
      "--- Deleted model: sm-ncf-model-2022-10-16-13-30-35\n",
      "--- Deleted endpoint_config: sm-ncf-model-2022-10-16-13-30-35-config\n"
     ]
    }
   ],
   "source": [
    "from inference_utils import delete_endpoint_detail\n",
    "delete_endpoint_detail(client = sm_client, \n",
    "                endpoint_name = new_endpoint_name,\n",
    "                is_delete=True, \n",
    "                is_del_model=True, \n",
    "                is_del_endconfig=True,\n",
    "                is_del_endpoint=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
